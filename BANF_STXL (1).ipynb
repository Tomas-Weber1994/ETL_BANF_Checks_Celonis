{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up connection to EMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from pandas.errors import EmptyDataError\n",
    "from pycelonis import get_celonis, pql\n",
    "from pycelonis.ems import ColumnTransport, ColumnType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-02 09:13:49,991] INFO: No `base_url` given. Using environment variable 'CELONIS_URL'\n",
      "[2023-10-02 09:13:49,992] INFO: No `api_token` given. Using environment variable 'CELONIS_API_TOKEN'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-02 09:13:50,120] WARNING: KeyType is not set. Defaulted to 'APP_KEY'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-02 09:13:50,140] INFO: Initial connect successful! PyCelonis Version: 2.4.1\n",
      "[2023-10-02 09:13:50,173] INFO: `package-manager` permissions: ['$ACCESS_CHILD']\n",
      "[2023-10-02 09:13:50,174] INFO: `workflows` permissions: []\n",
      "[2023-10-02 09:13:50,174] INFO: `task-mining` permissions: []\n",
      "[2023-10-02 09:13:50,175] INFO: `action-engine` permissions: []\n",
      "[2023-10-02 09:13:50,175] INFO: `team` permissions: []\n",
      "[2023-10-02 09:13:50,176] INFO: `process-repository` permissions: []\n",
      "[2023-10-02 09:13:50,176] INFO: `process-analytics` permissions: []\n",
      "[2023-10-02 09:13:50,177] INFO: `transformation-center` permissions: []\n",
      "[2023-10-02 09:13:50,177] INFO: `storage-manager` permissions: []\n",
      "[2023-10-02 09:13:50,178] INFO: `event-collection` permissions: ['USE_ALL_DATA_MODELS', '$ACCESS_CHILD', 'CREATE_DATA_POOL', 'EDIT_ALL_DATA_POOLS']\n",
      "[2023-10-02 09:13:50,178] INFO: `user-provisioning` permissions: []\n",
      "[2023-10-02 09:13:50,179] INFO: `ml-workbench` permissions: []\n"
     ]
    }
   ],
   "source": [
    "celonis = get_celonis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pool = celonis.data_integration.get_data_pools().find(\"P2P\")\n",
    "data_pool = celonis.data_integration.get_data_pool(data_pool.id)\n",
    "data_model = data_pool.get_data_models().find('BANF (STXL table) for Python')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of initials dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-02 09:13:51,456] INFO: Successfully created data export with id '99b7346b-3bd9-4a43-aead-dfe3b56bbe32'\n",
      "[2023-10-02 09:13:51,457] INFO: Wait for execution of data export with id '99b7346b-3bd9-4a43-aead-dfe3b56bbe32'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.local/lib/python3.8/site-packages/pycelonis/utils/deprecation.py:6: UserWarning: Deprecation\n",
      "The method `data_model.export_data_frame` has been deprecated and will be removed in future versions.\n",
      "Please use SaolaPy from now on to export PQL queries:\n",
      "\n",
      "\timport pycelonis.pql as pql\n",
      "\n",
      "\tdf = pql.DataFrame.from_pql(query, data_model=data_model)\n",
      "\tdf.head()\n",
      "\n",
      "For more information on SaolaPy, please visit https://celonis.github.io/pycelonis/2.4.1/tutorials/executed/05_saolapy/01_saolapy_quickstart/\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4bf5129311412398ec0ada493f8f8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-02 09:13:51,510] INFO: Export result chunks for data export with id '99b7346b-3bd9-4a43-aead-dfe3b56bbe32'\n"
     ]
    }
   ],
   "source": [
    "query = pql.PQL()\n",
    "query += pql.PQLColumn(query = \"BNF_STXL.MANDT\", name = \"MANDT\")\n",
    "query += pql.PQLColumn(query = \"BNF_STXL.BANFN\", name = \"BANFN\")\n",
    "query += pql.PQLColumn(query = \"BNF_STXL.BNFPO\", name = \"BNFPO\")\n",
    "query += pql.PQLColumn(query = \"BNF_STXL.DATAORIGIN\", name = \"DATAORIGIN\")\n",
    "query += pql.PQLColumn(query = \"BNF_STXL.TDNAME\", name = \"TDNAME\")\n",
    "query += pql.PQLColumn(query = \"BNF_STXL.TEXT_\", name = \"TEXT_\")\n",
    "\n",
    "# FILTERS - already in SQL table:\n",
    "# EBAN.BLCKD = '1' AND  BLCKT = 'Logbuch in Positionsnotiz'\n",
    "\n",
    "df = data_model.export_data_frame(query)\n",
    "    \n",
    "if df.empty:\n",
    "    print('DataFrame contains no data empty. Script failed!')\n",
    "    raise EmptyDataError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_202650/3091664820.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered.drop('condition_check', axis = 1, inplace = True)\n"
     ]
    }
   ],
   "source": [
    "def remove_logbuch_info(text: str) -> str:\n",
    "    \"\"\"\n",
    "    removing logbuch redundant info\n",
    "    :param _id: text content coming from STXL source table\n",
    "    :return: preprocessed text\n",
    "    \"\"\"\n",
    "    preprocessed_text = re.sub(\"#Logbuchende.*$\", '' , text) # excluding everything after first Logbuchende, instead of row above\n",
    "    \n",
    "    logbuch_pattern = re.compile(r\"#Logbuch.*?\\d{2}.\\d{2}.\\d{4}\")\n",
    "    text = re.sub(logbuch_pattern, '', preprocessed_text)\n",
    "    text += '#'\n",
    "    return text\n",
    "\n",
    "def check_conditions(text: str) -> bool:\n",
    "    \"\"\"\n",
    "    marking records which will be deleted further\n",
    "    redundant are all empty texts + texts not generated by a bot (no Logbuch and #\\d)\n",
    "    :text: preprocessed text after the removal of logbuch info\n",
    "    :return: boolean value of condition checks\n",
    "    \"\"\"\n",
    "    switch = True\n",
    "    if not text:\n",
    "        switch = False\n",
    "    elif 'Logbuch' not in text:\n",
    "        switch = False\n",
    "    elif '#1' not in text:\n",
    "        switch = False\n",
    "    return switch\n",
    "\n",
    "df['condition_check'] = df['TEXT_'].apply(check_conditions)\n",
    "df_filtered = df[df['condition_check'] == True]\n",
    "df_filtered.drop('condition_check', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regex_patterns() -> list:\n",
    "    \"\"\"\n",
    "    creating regex patterns from the defined strings \n",
    "    purpose is to match later only the specific information to the related string (column)\n",
    "    :return: list of regex patterns\n",
    "    \"\"\"\n",
    "    patterns = []\n",
    "    for string in strings_to_catch:\n",
    "        patterns.append(re.compile(fr\"(?<={string}).*?(?=#)\", re.IGNORECASE))\n",
    "    return patterns\n",
    "\n",
    "def create_new_columns(dataframe: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    assigning defined strings as new columns to the dataframe\n",
    "    :dataframe: input df\n",
    "    :return: dataframe with new columns (containing empty strings for now)\n",
    "    \"\"\"\n",
    "    for string in strings_to_catch:\n",
    "        if string.endswith(':'):\n",
    "            string = string[:-1]\n",
    "        dataframe[string] = ''\n",
    "    return dataframe\n",
    "\n",
    "def fill_column(text: str, pattern: re.Pattern) -> str:\n",
    "    \"\"\"\n",
    "    matching the specific information to the related column\n",
    "    :text: content of preprocessed text field\n",
    "    :return: text(str)\n",
    "    \"\"\"\n",
    "    result = ''\n",
    "    if match := pattern.search(text):\n",
    "        result = match.group()\n",
    "    return result\n",
    "\n",
    "def check_if_ok(input_text, column) -> str:\n",
    "    \"\"\"\n",
    "    final creation of the checks based on the conditions from business side\n",
    "    :input_text: string inside the specific column\n",
    "    :column: specified column of the dataframe\n",
    "    :return: final check (str)\n",
    "    \"\"\"\n",
    "    text = 'Not OK'\n",
    "    #print(input_text, column)\n",
    "    if input_text == '':                   # If check is not mentioned --> NULL \n",
    "        text = None\n",
    "    elif column in ['Leistungsnummer/0,01 €', 'Leistungsnummer/0 Menge', 'Vertragslaufzeit', 'Neue Leistungsnummer']:\n",
    "        if 'OK' in input_text:\n",
    "            text = 'OK'\n",
    "    elif column in ['Prüfung Bauadresse', 'Korrekte MSGK']:\n",
    "        if 'Ja' in input_text:\n",
    "            text = 'OK'\n",
    "    elif column == 'Leistungsnummer doppelt':\n",
    "        if 'Nein' in input_text:\n",
    "            text = 'OK'\n",
    "    elif column == 'Prüfung AVV':\n",
    "        if re.search(r\"(Nein)|(Ja, ADV erforderlich)|(Ja, ADV vorhanden)\", input_text):\n",
    "            text = 'OK'\n",
    "    elif column == 'Ausführungszeitraum':\n",
    "        if 'OK' in input_text or re.search(r\"Alt: \\d{2}.\\d{2}.\\d{4}, Liefertermin \\d{2}.\\d{2}.\\d{4}\", input_text):\n",
    "            text = 'OK'\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "strings_to_catch = [\n",
    "    'Leistungsnummer/0,01 €:',\n",
    "    'Leistungsnummer/0 Menge:',\n",
    "    'Ausführungszeitraum:',\n",
    "    'Prüfung AVV:',\n",
    "    'Prüfung Bauadresse:',\n",
    "    'Korrekte MSGK:',\n",
    "    'Leistungsnummer doppelt:',\n",
    "    'Vertragslaufzeit:', \n",
    "    'Neue Leistungsnummer:'              \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[re.compile(r'(?<=Leistungsnummer/0,01 €:).*?(?=#)', re.IGNORECASE|re.UNICODE),\n",
       " re.compile(r'(?<=Leistungsnummer/0 Menge:).*?(?=#)',\n",
       "            re.IGNORECASE|re.UNICODE),\n",
       " re.compile(r'(?<=Ausführungszeitraum:).*?(?=#)', re.IGNORECASE|re.UNICODE),\n",
       " re.compile(r'(?<=Prüfung AVV:).*?(?=#)', re.IGNORECASE|re.UNICODE),\n",
       " re.compile(r'(?<=Prüfung Bauadresse:).*?(?=#)', re.IGNORECASE|re.UNICODE),\n",
       " re.compile(r'(?<=Korrekte MSGK:).*?(?=#)', re.IGNORECASE|re.UNICODE),\n",
       " re.compile(r'(?<=Leistungsnummer doppelt:).*?(?=#)',\n",
       "            re.IGNORECASE|re.UNICODE),\n",
       " re.compile(r'(?<=Vertragslaufzeit:).*?(?=#)', re.IGNORECASE|re.UNICODE),\n",
       " re.compile(r'(?<=Neue Leistungsnummer:).*?(?=#)', re.IGNORECASE|re.UNICODE)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patterns = []\n",
    "for string in strings_to_catch:\n",
    "    patterns.append(re.compile(fr\"(?<={string}).*?(?=#)\", re.IGNORECASE))\n",
    "    \n",
    "patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns without ':' in the end\n",
    "columns = [column[:-1] for column in strings_to_catch]\n",
    "patterns = create_regex_patterns()\n",
    "\n",
    "df_copy = df_filtered.copy()\n",
    "df_copy = create_new_columns(df_copy)\n",
    "\n",
    "for column, pattern in zip(df_copy[columns], patterns):\n",
    "    df_copy[column] = df_copy.apply(lambda x: fill_column(x['TEXT_'], pattern),axis=1) \n",
    "\n",
    "for column in columns:\n",
    "    df_copy[column] = df_copy.apply(lambda x: check_if_ok(x[column], column),axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pushing the data into Celonis EMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_config = [\n",
    "    ColumnTransport(column_name=\"MANDT\", column_type=ColumnType.STRING, field_length=3),\n",
    "    ColumnTransport(column_name=\"BANFN\", column_type=ColumnType.STRING, field_length=10),\n",
    "    ColumnTransport(column_name=\"BNFPO\", column_type=ColumnType.STRING, field_length=5),\n",
    "    ColumnTransport(column_name=\"DATAORIGIN\", column_type=ColumnType.STRING, field_length=20),\n",
    "    ColumnTransport(column_name=\"TDNAME\", column_type=ColumnType.STRING, field_length=20),\n",
    "    ColumnTransport(column_name=\"TEXT_\", column_type=ColumnType.STRING, field_length=5000),\n",
    "    ColumnTransport(column_name=\"Leistungsnummer/0,01 €\", column_type=ColumnType.STRING, field_length=20),\n",
    "    ColumnTransport(column_name=\"Leistungsnummer/0 Menge\", column_type=ColumnType.STRING, field_length=20),\n",
    "    ColumnTransport(column_name=\"Ausführungszeitraum\", column_type=ColumnType.STRING, field_length=20),\n",
    "    ColumnTransport(column_name=\"Prüfung AVV\", column_type=ColumnType.STRING, field_length=20),\n",
    "    ColumnTransport(column_name=\"Prüfung Bauadresse\", column_type=ColumnType.STRING, field_length=20),\n",
    "    ColumnTransport(column_name=\"Korrekte MSGK\", column_type=ColumnType.STRING, field_length=20),\n",
    "    ColumnTransport(column_name=\"Leistungsnummer doppelt\", column_type=ColumnType.STRING, field_length=20),\n",
    "    ColumnTransport(column_name=\"Vertragslaufzeit\", column_type=ColumnType.STRING, field_length=20),\n",
    "    ColumnTransport(column_name=\"Neue Leistungsnummer\", column_type=ColumnType.STRING, field_length=20)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-02 09:13:52,533] INFO: Successfully created data push job with id 'cdc90070-d2b6-452c-99a2-dd8f7de292de'\n",
      "[2023-10-02 09:13:52,535] INFO: Add data frame as file chunks to data push job with id 'cdc90070-d2b6-452c-99a2-dd8f7de292de'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bca932951074f2c9f8aba3d920b9acf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-02 09:13:52,649] INFO: Successfully upserted file chunk to data push job with id 'cdc90070-d2b6-452c-99a2-dd8f7de292de'\n",
      "[2023-10-02 09:13:52,752] INFO: Successfully triggered execution for data push job with id 'cdc90070-d2b6-452c-99a2-dd8f7de292de'\n",
      "[2023-10-02 09:13:52,753] INFO: Wait for execution of data push job with id 'cdc90070-d2b6-452c-99a2-dd8f7de292de'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95d4edb3f83c4a9cbe8cf396cc01adf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-02 09:13:54,215] INFO: Successfully created table 'BNF_CHECKS_PYTHON' in data pool\n",
      "[2023-10-02 09:13:54,273] INFO: Successfully deleted data push job with id 'cdc90070-d2b6-452c-99a2-dd8f7de292de'\n",
      "[2023-10-02 09:13:55,038] INFO: Successfully started execution for job with id '47e1db96-d7d2-4f5f-841f-43a798a88174'\n",
      "[2023-10-02 09:13:55,038] INFO: Wait for execution of job with id '47e1db96-d7d2-4f5f-841f-43a798a88174'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c214b06ee4643b183feb6bbf95ee116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully pushed into PHB.\n"
     ]
    }
   ],
   "source": [
    "# push to EMS - (global workbench) and execute the datajob in global workbench to transform the table to production\n",
    "data_pool.create_table(table_name=\"BNF_CHECKS_PYTHON\",\n",
    "                   df=df_copy,\n",
    "                   drop_if_exists=True,\n",
    "                   column_config = column_config)  \n",
    "\n",
    "# force = True # resets all columns to str(80 chars)\n",
    "\n",
    "datajob = data_pool.get_jobs().find(\"Workbench for Python Pushing\")\n",
    "datajob.execute()\n",
    "print(\"Data successfully pushed into production.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-02 09:13:59,945] INFO: Successfully triggered data model reload for data model with id '107f8869-45a0-43f1-b64b-a47e91c1cb67'\n",
      "[2023-10-02 09:13:59,946] INFO: Wait for execution of data model reload for data model with id '107f8869-45a0-43f1-b64b-a47e91c1cb67'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b47087c342f485f965b85a153b5e51b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2023-10-02 09:15:27,518] WARNING: WARNING: Column [\"_CEL_BNF_ACTIVITIES\".\"ACTIVITY_EN\"] contains 19335 NULL values. The join between table \"V_AUTH_CONCEPT_MIDDLE\" and table \"EBAN\" did not find any matching rows. Please check your join definition and your data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BANF datamodel succesfully reloaded\n"
     ]
    }
   ],
   "source": [
    "data_model = data_pool.get_data_models().find('BANF(original_new) PHB')\n",
    "\n",
    "data_model.reload(wait=True)\n",
    "print(\"BANF datamodel succesfully reloaded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
